{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e7b143-d391-4eb0-9b47-809eb766de15",
   "metadata": {},
   "source": [
    "**Objective: Classify insurance claims as claims or not claims based on claims notes. Derive summaries and titles from claim notes\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c9fe926-6a30-4d7a-a2a4-86c8c065ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11397297-a899-44c7-a80b-40418b621a9a",
   "metadata": {},
   "source": [
    "**Dataset from huggingface**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77792f31-44a9-4a08-9e45-f8da3533f8be",
   "metadata": {},
   "source": [
    "**Import from the API or read the csv directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239c73a-49db-47cf-bed0-a7495530e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download insurance claims dataset from hugging face\n",
    "\n",
    "import requests\n",
    "\n",
    "#API endpoints for rows\n",
    "\n",
    "url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "\n",
    "#specify params\n",
    "\n",
    "params = {\n",
    "    \"dataset\": \"infinite-dataset-hub/TextClaimsDataset\",\n",
    "    \"config\": \"default\",\n",
    "    \"split\": \"train\",\n",
    "        \"offset\": 0,       # Starting row\n",
    "    \"length\": 100,     # Number of rows to fetch\n",
    "}\n",
    "\n",
    "#make GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "#check response status\n",
    "\n",
    "if response.status_code == 200:\n",
    "    #parse json and concert to dataframe\n",
    "\n",
    "    data=response.json()\n",
    "    rows=pd.DataFrame(data[\"rows\"])\n",
    "    print(rows.head())\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch rows: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3082be-f1d9-47c7-b465-b2ad51c7c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/infinite-dataset-hub/TextClaimsDataset/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc79b253-4be6-4d74-bd71-cf60bfbb3626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                                               Text        Label\n",
      "0     0  The policyholder reported a burglary at their ...        Claim\n",
      "1     1  I purchased a new phone and am not satisfied w...  Not a Claim\n",
      "2     2  An individual claimed to have been involved in...        Claim\n",
      "3     3  The user is asking for assistance with underst...  Not a Claim\n",
      "4     4  A homeowner filed a claim after their property...        Claim\n",
      "..  ...                                                ...          ...\n",
      "92   92  The insured party is seeking information on ho...  Not a Claim\n",
      "93   93  A claim for lost income has been filed by an e...        Claim\n",
      "94   94  An individual is asking about the claims proce...  Not a Claim\n",
      "95   96  There's a community event at the park this Sat...  Not a Claim\n",
      "96   99  The pet owner submitted a claim for their lost...        Claim\n",
      "\n",
      "[97 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e6522e-29ce-44fe-b3d1-b3b87b18eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx                                               Text        Label  \\\n",
      "0     0  The policyholder reported a burglary at their ...        Claim   \n",
      "1     1  I purchased a new phone and am not satisfied w...  Not a Claim   \n",
      "2     2  An individual claimed to have been involved in...        Claim   \n",
      "3     3  The user is asking for assistance with underst...  Not a Claim   \n",
      "4     4  A homeowner filed a claim after their property...        Claim   \n",
      "..  ...                                                ...          ...   \n",
      "92   92  The insured party is seeking information on ho...  Not a Claim   \n",
      "93   93  A claim for lost income has been filed by an e...        Claim   \n",
      "94   94  An individual is asking about the claims proce...  Not a Claim   \n",
      "95   96  There's a community event at the park this Sat...  Not a Claim   \n",
      "96   99  The pet owner submitted a claim for their lost...        Claim   \n",
      "\n",
      "    labels_binary  \n",
      "0               1  \n",
      "1               0  \n",
      "2               1  \n",
      "3               0  \n",
      "4               1  \n",
      "..            ...  \n",
      "92              0  \n",
      "93              1  \n",
      "94              0  \n",
      "95              0  \n",
      "96              1  \n",
      "\n",
      "[97 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def label_claims(text):\n",
    "    if text==\"Claim\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df[\"labels_binary\"] = df[\"Label\"].apply(label_claims).astype(int)\n",
    "\n",
    "print(df)\n",
    "\n",
    "#print(df[[\"Text\", \"labels\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f18227-9a4c-4c3d-a400-976d2b209b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all text entries are strings\n",
    "df[\"Text\"] = df[\"Text\"].astype(str).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a09266-76fb-4eb9-ad29-cb089a39ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fba487b-d594-4479-b3f2-de57c63739fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6064f693-c617-4262-b7bf-4ced1f6bbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the index numbers\n",
    "df['idx'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0620f36-4bf5-40da-92f0-0ee6feac958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "df = df[[\"Text\", \"labels_binary\"]]\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"Text\"], df[\"labels_binary\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfa5c89-4aac-40f9-a50f-aa414913d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to tensors (ensure labels are of type LongTensor)\n",
    "train_labels = torch.tensor(list(train_labels.values), dtype=torch.long)\n",
    "val_labels = torch.tensor(list(val_labels.values), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b01ed1-f776-4c60-8797-64b29ed5229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = ClaimsDataset(train_encodings, train_labels)\n",
    "val_dataset = ClaimsDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920e3c4e-d4f5-45be-9e87-fd1c8757e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\sukri\\anaconda3\\envs\\env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define training loop\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de11f4b-570c-41af-af3c-314414a4cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy: 0.9000, Precision: 1.0000, Recall: 0.8000, F1: 0.8889\n",
      "Epoch 2: Accuracy: 0.9500, Precision: 1.0000, Recall: 0.9000, F1: 0.9474\n",
      "Epoch 3: Accuracy: 0.9000, Precision: 0.9000, Recall: 0.9000, F1: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluation\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average=\"binary\")\n",
    "    print(f\"Epoch {epoch + 1}: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b18a5150-7ec8-4ae2-a375-3a7e859cc8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert\\\\tokenizer_config.json',\n",
       " './fine_tuned_bert\\\\special_tokens_map.json',\n",
       " './fine_tuned_bert\\\\vocab.txt',\n",
       " './fine_tuned_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "176b94e3-f5ec-4b4c-87ae-f948c55b6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Not a Claim (0)       0.90      0.90      0.90        10\n",
      "      Claim (1)       0.90      0.90      0.90        10\n",
      "\n",
      "       accuracy                           0.90        20\n",
      "      macro avg       0.90      0.90      0.90        20\n",
      "   weighted avg       0.90      0.90      0.90        20\n",
      "\n",
      "                                                Text  True_Label  \\\n",
      "0  A customer called to dispute a charge on their...           0   \n",
      "1          An individual wants to report a lost pet.           0   \n",
      "2  A claim for lost income has been filed by an e...           1   \n",
      "3  My friend recommended a new restaurant that se...           0   \n",
      "4  The insured reports that their home security s...           1   \n",
      "\n",
      "   Predicted_Label  \n",
      "0                1  \n",
      "1                0  \n",
      "2                1  \n",
      "3                0  \n",
      "4                1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "val_preds = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        # Move data to device (GPU or CPU)\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Get the predicted class (0 or 1)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_labels_list.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_labels_list, val_preds, target_names=[\"Not a Claim (0)\", \"Claim (1)\"]))\n",
    "\n",
    "# Optionally, print predictions alongside the actual texts\n",
    "val_results = pd.DataFrame({\n",
    "    \"Text\": val_texts.reset_index(drop=True),\n",
    "    \"True_Label\": val_labels_list,\n",
    "    \"Predicted_Label\": val_preds\n",
    "})\n",
    "print(val_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244200f-e199-4913-af39-e9d04879ad48",
   "metadata": {},
   "source": [
    "**Using GPT for Claim Summarization**\n",
    "*Objective: Summarize the insurance claim for customer communication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103657a9-2643-41ce-b861-85ed9fce5b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Text_Length\n",
      "0  The policyholder reported a burglary at their ...           94\n",
      "1  I purchased a new phone and am not satisfied w...           66\n",
      "2  An individual claimed to have been involved in...          100\n",
      "3  The user is asking for assistance with underst...           85\n",
      "4  A homeowner filed a claim after their property...           81\n"
     ]
    }
   ],
   "source": [
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x))\n",
    "print(df[['Text', 'Text_Length']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff8d25c-bbd0-4a58-8e53-d5cc93d86bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Text_Word_Count\n",
      "0  The policyholder reported a burglary at their ...               16\n",
      "1  I purchased a new phone and am not satisfied w...               12\n",
      "2  An individual claimed to have been involved in...               17\n",
      "3  The user is asking for assistance with underst...               12\n",
      "4  A homeowner filed a claim after their property...               14\n"
     ]
    }
   ],
   "source": [
    "df['Text_Word_Count'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "print(df[['Text', 'Text_Word_Count']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37a7db3b-3c6c-4cb5-ab1a-91905e92eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  The policyholder reported a burglary at their ...   \n",
      "1  I purchased a new phone and am not satisfied w...   \n",
      "2  An individual claimed to have been involved in...   \n",
      "3  The user is asking for assistance with underst...   \n",
      "4  A homeowner filed a claim after their property...   \n",
      "\n",
      "                                             Summary  \n",
      "0  The policyholder reported a burglary at their ...  \n",
      "1  I purchased a new phone and am not satisfied w...  \n",
      "2  An individual claimed to have been involved in...  \n",
      "3  The user is asking for assistance with underst...  \n",
      "4  A homeowner filed a claim after their property...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load Pretrained GPT2 Model and Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Define pad_token_id explicitly since GPT2 doesn't have a pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate summaries for a given claim\n",
    "def generate_summary(claim, max_length=50):\n",
    "    # Encode the input claim (the text to be summarized)\n",
    "    inputs = tokenizer.encode(claim, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "\n",
    "    # Ensure attention mask is set correctly\n",
    "    attention_mask[inputs == tokenizer.pad_token_id] = 0\n",
    "    \n",
    "    # Generate summary using the GPT-2 model\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        attention_mask=attention_mask,  # Pass attention mask to the model\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Explicitly set pad_token_id\n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1,  # Generate only one summary\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "        temperature=0.7,  # Controls randomness of predictions\n",
    "        top_k=50,  # Top-k sampling for diversity\n",
    "        top_p=0.95,  # Top-p (nucleus) sampling for diversity\n",
    "        do_sample=True,  # Enable sampling (as opposed to greedy search)\n",
    "    )\n",
    "    \n",
    "    # Decode the output summary\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the summary is shorter than the input (if needed)\n",
    "    #if len(summary.split()) > len(claim.split()):\n",
    "        #summary = \"Summary too long, trimming...\"\n",
    "\n",
    "    return summary.strip()\n",
    "\n",
    "# Apply summarization to each row in the 'Text' column\n",
    "df['Summary'] = df['Text'].apply(lambda x: generate_summary(x))\n",
    "\n",
    "# Show the resulting dataframe with summaries\n",
    "print(df[['Text', 'Summary']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abb6d194-d24c-4c9b-8c2b-200e59aca17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  The policyholder reported a burglary at their ...   \n",
      "1  I purchased a new phone and am not satisfied w...   \n",
      "2  An individual claimed to have been involved in...   \n",
      "3  The user is asking for assistance with underst...   \n",
      "4  A homeowner filed a claim after their property...   \n",
      "\n",
      "                                             Summary  Summary_Length  \n",
      "0  The policyholder reported a burglary at their ...             149  \n",
      "1  I purchased a new phone and am not satisfied w...             221  \n",
      "2  An individual claimed to have been involved in...             249  \n",
      "3  The user is asking for assistance with underst...             256  \n",
      "4  A homeowner filed a claim after their property...             251  \n"
     ]
    }
   ],
   "source": [
    "df['Summary_Length'] = df['Summary'].apply(lambda x: len(x))\n",
    "print(df[['Text', 'Summary', 'Summary_Length']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11020c26-ac91-4074-abbf-298f644325e2",
   "metadata": {},
   "source": [
    "**Train BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6847da2-b900-4249-a917-7b44b10beb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)  # Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3463318-9f55-4202-8c9f-2b8ee589f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BART Model and Tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6ff6bc0-64e4-41b9-8688-ed06c873a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Each Row in the 'text' Column\n",
    "def summarize_text(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=10, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22c2b7c2-def0-4724-9486-fef65918dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  labels_binary  \\\n",
      "0   The policyholder reported a burglary at their ...              1   \n",
      "1   I purchased a new phone and am not satisfied w...              0   \n",
      "2   An individual claimed to have been involved in...              1   \n",
      "3   The user is asking for assistance with underst...              0   \n",
      "4   A homeowner filed a claim after their property...              1   \n",
      "..                                                ...            ...   \n",
      "92  The insured party is seeking information on ho...              0   \n",
      "93  A claim for lost income has been filed by an e...              1   \n",
      "94  An individual is asking about the claims proce...              0   \n",
      "95  There's a community event at the park this Sat...              0   \n",
      "96  The pet owner submitted a claim for their lost...              1   \n",
      "\n",
      "    Text_Length  Text_Word_Count                                      summary  \n",
      "0            94               16      The policyholder reported a burglary at  \n",
      "1            66               12                     summarize: I purchased a  \n",
      "2           100               17  An individual claimed to have been involved  \n",
      "3            85               12                       summarize: The user is  \n",
      "4            81               14        A homeowner filed a claim after their  \n",
      "..          ...              ...                                          ...  \n",
      "92           80               13  The insured party is seeking information on  \n",
      "93          114               24             A claim for lost income has been  \n",
      "94           87               13     An individual is asking about the claims  \n",
      "95           52                9             There's a community event at the  \n",
      "96          110               19     The dog was stolen during a neighborhood  \n",
      "\n",
      "[97 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df[\"summary\"] = df[\"Text\"].apply(summarize_text)\n",
    "\n",
    "# Print Summaries\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "246511f7-9912-4b9d-b45d-3930968d7c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  The policyholder reported a burglary at their ...   \n",
      "1  I purchased a new phone and am not satisfied w...   \n",
      "2  An individual claimed to have been involved in...   \n",
      "3  The user is asking for assistance with underst...   \n",
      "4  A homeowner filed a claim after their property...   \n",
      "\n",
      "                                       summary  Summary_Word_Count  \n",
      "0      The policyholder reported a burglary at                   6  \n",
      "1                     summarize: I purchased a                   4  \n",
      "2  An individual claimed to have been involved                   7  \n",
      "3                       summarize: The user is                   4  \n",
      "4        A homeowner filed a claim after their                   7  \n"
     ]
    }
   ],
   "source": [
    "df['Summary_Word_Count'] = df['summary'].apply(lambda x: len(x.split()))\n",
    "print(df[['Text','summary', 'Summary_Word_Count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f5382-f2e3-4351-aa90-4320b36472af",
   "metadata": {},
   "source": [
    "**Train T5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe5d7e-4654-44b0-bdc4-771e8c27fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44430b91-20e1-4b98-a161-0caf74831566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained T5 Model and Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46946e79-bd1f-4041-b177-7624c9c3f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  labels_binary  \\\n",
      "0   The policyholder reported a burglary at their ...              1   \n",
      "1   I purchased a new phone and am not satisfied w...              0   \n",
      "2   An individual claimed to have been involved in...              1   \n",
      "3   The user is asking for assistance with underst...              0   \n",
      "4   A homeowner filed a claim after their property...              1   \n",
      "..                                                ...            ...   \n",
      "92  The insured party is seeking information on ho...              0   \n",
      "93  A claim for lost income has been filed by an e...              1   \n",
      "94  An individual is asking about the claims proce...              0   \n",
      "95  There's a community event at the park this Sat...              0   \n",
      "96  The pet owner submitted a claim for their lost...              1   \n",
      "\n",
      "    Text_Length  Text_Word_Count  \\\n",
      "0            94               16   \n",
      "1            66               12   \n",
      "2           100               17   \n",
      "3            85               12   \n",
      "4            81               14   \n",
      "..          ...              ...   \n",
      "92           80               13   \n",
      "93          114               24   \n",
      "94           87               13   \n",
      "95           52                9   \n",
      "96          110               19   \n",
      "\n",
      "                                              summary  Summary_Word_Count  \n",
      "0           policyholder reported a burglary at their                   6  \n",
      "1                      i purchased a new phone and am                   4  \n",
      "2       individual claimed to have been involved in a                   7  \n",
      "3   the user is asking for assistance with underst...                   4  \n",
      "4               a homeowner filed a claim after their                   7  \n",
      "..                                                ...                 ...  \n",
      "92  the insured party is seeking information on ho...                   7  \n",
      "93             an employee was injured on the job and                   7  \n",
      "94  an individual is asking about the claims proce...                   7  \n",
      "95                   there's a community event at the                   6  \n",
      "96          the pet owner submitted a claim for their                   7  \n",
      "\n",
      "[97 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Summarize Each Row in the 'text' Column\n",
    "def summarize_text(text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=10, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "df[\"summary\"] = df[\"Text\"].apply(summarize_text)\n",
    "\n",
    "# Print Summaries\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c39f5-b08c-4b90-abb2-19223d695e4d",
   "metadata": {},
   "source": [
    "**Extracting title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53524a02-3871-4ca3-a1ed-2d55d3b44d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  \\\n",
      "0   The policyholder reported a burglary at their ...   \n",
      "1   I purchased a new phone and am not satisfied w...   \n",
      "2   An individual claimed to have been involved in...   \n",
      "3   The user is asking for assistance with underst...   \n",
      "4   A homeowner filed a claim after their property...   \n",
      "..                                                ...   \n",
      "92  The insured party is seeking information on ho...   \n",
      "93  A claim for lost income has been filed by an e...   \n",
      "94  An individual is asking about the claims proce...   \n",
      "95  There's a community event at the park this Sat...   \n",
      "96  The pet owner submitted a claim for their lost...   \n",
      "\n",
      "                                     Title  \n",
      "0               home and submitted a claim  \n",
      "1                       a new phone and am  \n",
      "2                  have been involved in a  \n",
      "3   The user is asking for assistance with  \n",
      "4      after their property was damaged by  \n",
      "..                                     ...  \n",
      "92        is seeking information on how to  \n",
      "93           lost income has been filed by  \n",
      "94              the process for a business  \n",
      "95                a community event at the  \n",
      "96             owner submitted a claim for  \n",
      "\n",
      "[97 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Summarize Each Row in the 'text' Column\n",
    "def generate_title(text):\n",
    "    inputs = tokenizer.encode(\"generate a concise claim title: \", text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, \n",
    "                                 max_length=8, \n",
    "                                 min_length=5, \n",
    "                                 length_penalty=2.0, # Encourage concise output\n",
    "                                 num_beams=6,   ## More beams for better exploration\n",
    "                                 early_stopping=True) \n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "df[\"Title\"] = df[\"Text\"].apply(generate_title)\n",
    "\n",
    "# Print Titles\n",
    "print(df[[\"Text\", \"Title\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
